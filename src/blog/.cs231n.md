---
title: cs231n
date: 2025-07-09
location: "Abu Dhabi, UAE"
tags: ["posts"]
layout: article.njk
---
## Image Classification

> the Image Classification problem is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.
> 
> _from cs231n_

### Nearest Neighbor Classifier

- 将图片向量化为 $I$ 后，通过**最小化**距离函数 $d (I_1, I_2)$，找出与目标图片最为相似的图片，将目标图片归类于该类别。
- The size of inputs (an image) is $N \times Height \times Width \times 3$, where $N$ is the amount of images，$H \times W$ is the amount of pixels，$3$ represents RGB color channels.

```python
Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide
# flatten out all images to be one-dimensional
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072
```

```python
nn = NearestNeighbor() # create a Nearest Neighbor classifier class
nn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels
Yte_predict = nn.predict(Xte_rows) # predict labels on the test images
# and now print the classification accuracy, which is the average number
# of examples that are correctly predicted (i.e. label matches)
print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )
```

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
```

### k - Nearest Neighbor Classifier
- 在前 $k$ 与目标图片相似的图片里，选取数量最多的标签 $cnt \in [1, k]$，记为图片类别
- k - Nearest Neighbor Classifier has better generalization comparing to the Nearest Neighbor Classifier, because it considers more training data.

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X, k):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the top k nearest training images to the i'th test image
      # find the most voted label
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
	  top_k_indices = np.argpartition(distances, k - 1)[:k]
	  top_k_labels = self.ytr[top_k_indices]
	  
	  unique_vals, counts = np.unique(top_k_labels, return_counts=True)
	  most_freq_val = unique_vals[np.argmax(counts)]
	  
      Ypred[i] = most_freq_val # predict the label of the most voted label

    return Ypred
```

### Validation sets for Hyperparameter tuning
> Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end. Otherwise, the very real danger is that you may tune your hyperparameters to work well on the test set, but if you were to deploy your model you could see a significantly reduced performance. In practice, we would say that you **overfit** to the test set.
> *from cs231n*

- **注意：** 这里 Otherwise 后面的理由同样是“使用单一校验集”的弊端。
- **重点： 用单一测试集调校参数让模型适应测试集；同理，用单一校验集调校参数让模型适应校验集；交叉校验才是合理的方法。**
- cross validation splits training set into several segments, then uses each of them as validation set to evaluate performance and finally average the performance across the different folds.
```python
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation_accuracies.append((k, acc))
```
 ##### Further Reading
 [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)
 [Recognizing and Learning Object Categories](https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html)

### Linear Classification

$$L = \frac{1}{N} \sum_{i} \, L_i \, ( f (\mathbf{x_i}, W) \, , \, y_i)$$

 Abbreviating $f(\mathbf{x_i}, W)$ to $s$, in which the j-th element $s_j = f(\mathbf{x_i}, W)_j$ is the score of the j-th class.
#### Score Function
> The loss function quantifies our unhappiness with predictions on the training set

$$
f ( \mathbf{x_i}, W, \mathbf{b} ) = W \mathbf{x_i} + \mathbf{b}
$$

For elegance, we extend $W$ by one column that is $\mathbf{b}$ and $\mathbf{x_i}$ by one dimension that always holds the constant $1$. Them, the score function will be simplified to:

$$
f ( \mathbf{x_i} , W ) = W \mathbf{x_i}
$$

#### Hinge Loss Function

$$
L_i = \sum_{j \neq y_i} \texttt{max} (0, s_j - s_{y_i} + \Delta)
$$

where $\Delta$ is the least acceptable difference between the score of the correct class $s_{y_i}$ and the score of each other classes $s_j$ .
This loss function is called hinge loss. Another hinge loss function called squared hinge loss function penalizes violated margins more strongly.


#### Softmax Classifier (Multinomial Logistic Regression)
Softmax classifier encourages infinite values of the correct class

$$
L_i = - \log (\frac{e^{s_{y_i}}}{\sum_{j} e^{s_j}}) = - e^{s_{y_i}} + \log (\sum_{j} e^{s_j})
$$

#### Regularization
Regularization is designed to avoid overfitting - by restricting the weight matrix $W$ to a simpler form, following the rule of Occam's Razor.
The full loss function is:
$$
L = \frac{1}{N} \sum_{i} \, L_i \, + \, \lambda R(W)
$$
A common regularization penalty is the squared **L2** norm, which discourage large weights:
$$
R(W) = \sum_{k} \sum_{l} W_{k, l}^2
$$
Here is the loss function (without regularization) implemented in Python, in unvectorized, half-vectorized, and fully-vectorized form:
```python
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in range(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def L(X, y, W):
  """
  fully-vectorized implementation :
  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
  - y is array of integers specifying correct class (e.g. 50,000-D array)
  - W are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment
  
  # Solution
  delta = 1.0
  scores = W.dot(x)
  # scores[y, np.arange(y.shape[0])] is the socres at y-th positions
  # scores[y, np.arange(y.shape[0])] and delta broadcast to
  # calculate with matrix scores
  margins = np.maximum(0, scores - scores[y, np.arange(y.shape[0])] + delta)
  margins[y, np.arange(y.shape[0])] = 0
  # sum losses
  loss = np.sum(L[:, np.arange(y.shape[0])])
  return loss
```

### Backpropagation
In order to simplify coding and avoid complex derivatives, a common way is to construct a computational graph representing the loss function, where each node is an operator with a few inputs. In this way, the loss function can be computed forwardly, and gradients can be computed backwardly.
>The gradients can be thought of as flowing backwards through the circuit.

$$
gradient_{so far} \, = \, gradient_{local} \, \times \, gradient_{upstream}
$$
**Pay attention to the shape of derivatives. Always check the shape.**
#### Tips for coding
1. **Cache forward pass variables**. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.

2. **Gradients add up at forks**. The forward expression involves the variables **x,y** multiple times, so when we perform backpropagation we must be careful to use `+=` instead of `=` to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the _multivariable chain rule_ in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.

### Notes for Partial Derivative
$$f (x, y, z)$$

$$x(u), \, y(u), \, z(u)$$

$$\frac{\partial f}{\partial u} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u} + \frac{\partial f}{\partial z} \frac{\partial z}{\partial u}$$
